{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7789799072642967,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.030911901081916538,
      "grad_norm": 2.5053181648254395,
      "learning_rate": 2.7551020408163265e-05,
      "loss": 2.9863,
      "step": 10
    },
    {
      "epoch": 0.061823802163833076,
      "grad_norm": 2.2006359100341797,
      "learning_rate": 5.816326530612244e-05,
      "loss": 2.9762,
      "step": 20
    },
    {
      "epoch": 0.09273570324574962,
      "grad_norm": 2.1640498638153076,
      "learning_rate": 8.877551020408162e-05,
      "loss": 2.5889,
      "step": 30
    },
    {
      "epoch": 0.12364760432766615,
      "grad_norm": 3.0290067195892334,
      "learning_rate": 0.0001193877551020408,
      "loss": 2.3387,
      "step": 40
    },
    {
      "epoch": 0.1545595054095827,
      "grad_norm": 4.829334259033203,
      "learning_rate": 0.00015,
      "loss": 1.8429,
      "step": 50
    },
    {
      "epoch": 0.18547140649149924,
      "grad_norm": 4.826037883758545,
      "learning_rate": 0.00018061224489795917,
      "loss": 1.3079,
      "step": 60
    },
    {
      "epoch": 0.21638330757341576,
      "grad_norm": 4.823958396911621,
      "learning_rate": 0.00021122448979591835,
      "loss": 0.7905,
      "step": 70
    },
    {
      "epoch": 0.2472952086553323,
      "grad_norm": 3.7636349201202393,
      "learning_rate": 0.00024183673469387753,
      "loss": 0.4835,
      "step": 80
    },
    {
      "epoch": 0.2782071097372488,
      "grad_norm": 1.2146703004837036,
      "learning_rate": 0.0002724489795918367,
      "loss": 0.2049,
      "step": 90
    },
    {
      "epoch": 0.3091190108191654,
      "grad_norm": 1.7580093145370483,
      "learning_rate": 0.00029965675057208233,
      "loss": 0.1429,
      "step": 100
    },
    {
      "epoch": 0.3400309119010819,
      "grad_norm": 1.0552477836608887,
      "learning_rate": 0.00029622425629290616,
      "loss": 0.0961,
      "step": 110
    },
    {
      "epoch": 0.37094281298299847,
      "grad_norm": 1.1627330780029297,
      "learning_rate": 0.00029279176201373,
      "loss": 0.1416,
      "step": 120
    },
    {
      "epoch": 0.401854714064915,
      "grad_norm": 1.4241249561309814,
      "learning_rate": 0.00028935926773455376,
      "loss": 0.1317,
      "step": 130
    },
    {
      "epoch": 0.4327666151468315,
      "grad_norm": 0.5940550565719604,
      "learning_rate": 0.00028592677345537753,
      "loss": 0.1311,
      "step": 140
    },
    {
      "epoch": 0.46367851622874806,
      "grad_norm": 0.516052782535553,
      "learning_rate": 0.00028249427917620136,
      "loss": 0.0804,
      "step": 150
    },
    {
      "epoch": 0.4945904173106646,
      "grad_norm": 3.81244158744812,
      "learning_rate": 0.00027906178489702513,
      "loss": 0.1016,
      "step": 160
    },
    {
      "epoch": 0.5255023183925811,
      "grad_norm": 2.4867682456970215,
      "learning_rate": 0.00027562929061784896,
      "loss": 0.0963,
      "step": 170
    },
    {
      "epoch": 0.5564142194744977,
      "grad_norm": 0.5250312089920044,
      "learning_rate": 0.00027219679633867273,
      "loss": 0.0779,
      "step": 180
    },
    {
      "epoch": 0.5873261205564142,
      "grad_norm": 0.6957187056541443,
      "learning_rate": 0.0002687643020594965,
      "loss": 0.0791,
      "step": 190
    },
    {
      "epoch": 0.6182380216383307,
      "grad_norm": 2.935047149658203,
      "learning_rate": 0.00026533180778032033,
      "loss": 0.0773,
      "step": 200
    },
    {
      "epoch": 0.6491499227202473,
      "grad_norm": 0.6020082831382751,
      "learning_rate": 0.00026189931350114416,
      "loss": 0.071,
      "step": 210
    },
    {
      "epoch": 0.6800618238021638,
      "grad_norm": 3.674006938934326,
      "learning_rate": 0.00025846681922196793,
      "loss": 0.1044,
      "step": 220
    },
    {
      "epoch": 0.7109737248840804,
      "grad_norm": 0.3159022033214569,
      "learning_rate": 0.00025503432494279176,
      "loss": 0.0825,
      "step": 230
    },
    {
      "epoch": 0.7418856259659969,
      "grad_norm": 1.3234699964523315,
      "learning_rate": 0.00025160183066361553,
      "loss": 0.0819,
      "step": 240
    },
    {
      "epoch": 0.7727975270479135,
      "grad_norm": 0.6416029334068298,
      "learning_rate": 0.0002481693363844393,
      "loss": 0.0736,
      "step": 250
    },
    {
      "epoch": 0.80370942812983,
      "grad_norm": 0.7815288305282593,
      "learning_rate": 0.00024473684210526314,
      "loss": 0.0841,
      "step": 260
    },
    {
      "epoch": 0.8346213292117465,
      "grad_norm": 0.4687913954257965,
      "learning_rate": 0.00024130434782608694,
      "loss": 0.0738,
      "step": 270
    },
    {
      "epoch": 0.865533230293663,
      "grad_norm": 0.7434468269348145,
      "learning_rate": 0.0002378718535469107,
      "loss": 0.0751,
      "step": 280
    },
    {
      "epoch": 0.8964451313755796,
      "grad_norm": 0.585371732711792,
      "learning_rate": 0.00023443935926773454,
      "loss": 0.077,
      "step": 290
    },
    {
      "epoch": 0.9273570324574961,
      "grad_norm": 0.3029510974884033,
      "learning_rate": 0.00023100686498855834,
      "loss": 0.0725,
      "step": 300
    },
    {
      "epoch": 0.9582689335394127,
      "grad_norm": 2.6466426849365234,
      "learning_rate": 0.0002275743707093821,
      "loss": 0.0703,
      "step": 310
    },
    {
      "epoch": 0.9891808346213292,
      "grad_norm": 0.6731992959976196,
      "learning_rate": 0.00022414187643020594,
      "loss": 0.075,
      "step": 320
    },
    {
      "epoch": 1.01854714064915,
      "grad_norm": 0.4367735683917999,
      "learning_rate": 0.00022070938215102974,
      "loss": 0.0827,
      "step": 330
    },
    {
      "epoch": 1.0494590417310665,
      "grad_norm": 0.3752140700817108,
      "learning_rate": 0.00021727688787185354,
      "loss": 0.0756,
      "step": 340
    },
    {
      "epoch": 1.080370942812983,
      "grad_norm": 0.4563840627670288,
      "learning_rate": 0.0002138443935926773,
      "loss": 0.0617,
      "step": 350
    },
    {
      "epoch": 1.1112828438948996,
      "grad_norm": 0.4727632999420166,
      "learning_rate": 0.00021041189931350114,
      "loss": 0.0678,
      "step": 360
    },
    {
      "epoch": 1.1421947449768162,
      "grad_norm": 0.3336821496486664,
      "learning_rate": 0.00020697940503432494,
      "loss": 0.0675,
      "step": 370
    },
    {
      "epoch": 1.1731066460587325,
      "grad_norm": 0.4574357271194458,
      "learning_rate": 0.0002035469107551487,
      "loss": 0.0791,
      "step": 380
    },
    {
      "epoch": 1.2040185471406493,
      "grad_norm": 0.38457587361335754,
      "learning_rate": 0.00020011441647597254,
      "loss": 0.0676,
      "step": 390
    },
    {
      "epoch": 1.2349304482225656,
      "grad_norm": 0.3978036046028137,
      "learning_rate": 0.00019668192219679634,
      "loss": 0.0659,
      "step": 400
    },
    {
      "epoch": 1.2658423493044824,
      "grad_norm": 0.3931961953639984,
      "learning_rate": 0.0001932494279176201,
      "loss": 0.0615,
      "step": 410
    },
    {
      "epoch": 1.2967542503863987,
      "grad_norm": 0.29105767607688904,
      "learning_rate": 0.00018981693363844391,
      "loss": 0.0711,
      "step": 420
    },
    {
      "epoch": 1.3276661514683152,
      "grad_norm": 0.21179573237895966,
      "learning_rate": 0.00018638443935926774,
      "loss": 0.0674,
      "step": 430
    },
    {
      "epoch": 1.3585780525502318,
      "grad_norm": 0.3182602822780609,
      "learning_rate": 0.00018295194508009151,
      "loss": 0.0719,
      "step": 440
    },
    {
      "epoch": 1.3894899536321483,
      "grad_norm": 0.3031361997127533,
      "learning_rate": 0.00017951945080091531,
      "loss": 0.0621,
      "step": 450
    },
    {
      "epoch": 1.4204018547140649,
      "grad_norm": 0.34649112820625305,
      "learning_rate": 0.00017608695652173914,
      "loss": 0.0648,
      "step": 460
    },
    {
      "epoch": 1.4513137557959814,
      "grad_norm": 0.344074010848999,
      "learning_rate": 0.00017265446224256292,
      "loss": 0.0625,
      "step": 470
    },
    {
      "epoch": 1.482225656877898,
      "grad_norm": 0.42951932549476624,
      "learning_rate": 0.00016922196796338672,
      "loss": 0.0717,
      "step": 480
    },
    {
      "epoch": 1.5131375579598145,
      "grad_norm": 0.47707992792129517,
      "learning_rate": 0.00016578947368421052,
      "loss": 0.067,
      "step": 490
    },
    {
      "epoch": 1.544049459041731,
      "grad_norm": 0.3125123679637909,
      "learning_rate": 0.00016235697940503432,
      "loss": 0.0645,
      "step": 500
    },
    {
      "epoch": 1.5749613601236476,
      "grad_norm": 0.3954704999923706,
      "learning_rate": 0.00015892448512585812,
      "loss": 0.0682,
      "step": 510
    },
    {
      "epoch": 1.6058732612055642,
      "grad_norm": 0.39326903223991394,
      "learning_rate": 0.00015549199084668192,
      "loss": 0.0721,
      "step": 520
    },
    {
      "epoch": 1.6367851622874807,
      "grad_norm": 0.33062371611595154,
      "learning_rate": 0.0001520594965675057,
      "loss": 0.064,
      "step": 530
    },
    {
      "epoch": 1.6676970633693973,
      "grad_norm": 0.49347013235092163,
      "learning_rate": 0.00014862700228832952,
      "loss": 0.0665,
      "step": 540
    },
    {
      "epoch": 1.6986089644513136,
      "grad_norm": 0.3245674669742584,
      "learning_rate": 0.0001451945080091533,
      "loss": 0.0586,
      "step": 550
    },
    {
      "epoch": 1.7295208655332304,
      "grad_norm": 0.1959872841835022,
      "learning_rate": 0.0001417620137299771,
      "loss": 0.0748,
      "step": 560
    },
    {
      "epoch": 1.7604327666151467,
      "grad_norm": 0.33115193247795105,
      "learning_rate": 0.00013832951945080092,
      "loss": 0.0625,
      "step": 570
    },
    {
      "epoch": 1.7913446676970635,
      "grad_norm": 0.6009829640388489,
      "learning_rate": 0.0001348970251716247,
      "loss": 0.061,
      "step": 580
    },
    {
      "epoch": 1.8222565687789798,
      "grad_norm": 0.2432236522436142,
      "learning_rate": 0.0001314645308924485,
      "loss": 0.0659,
      "step": 590
    },
    {
      "epoch": 1.8531684698608966,
      "grad_norm": 0.20650982856750488,
      "learning_rate": 0.0001280320366132723,
      "loss": 0.0636,
      "step": 600
    },
    {
      "epoch": 1.8840803709428129,
      "grad_norm": 0.33077776432037354,
      "learning_rate": 0.0001245995423340961,
      "loss": 0.0684,
      "step": 610
    },
    {
      "epoch": 1.9149922720247297,
      "grad_norm": 0.2142455130815506,
      "learning_rate": 0.0001211670480549199,
      "loss": 0.0648,
      "step": 620
    },
    {
      "epoch": 1.945904173106646,
      "grad_norm": 0.33556729555130005,
      "learning_rate": 0.00011773455377574371,
      "loss": 0.0658,
      "step": 630
    },
    {
      "epoch": 1.9768160741885628,
      "grad_norm": 0.33082425594329834,
      "learning_rate": 0.0001143020594965675,
      "loss": 0.0644,
      "step": 640
    },
    {
      "epoch": 2.006182380216383,
      "grad_norm": 0.24599210917949677,
      "learning_rate": 0.00011086956521739128,
      "loss": 0.0688,
      "step": 650
    },
    {
      "epoch": 2.0370942812983,
      "grad_norm": 0.266634076833725,
      "learning_rate": 0.0001074370709382151,
      "loss": 0.0702,
      "step": 660
    },
    {
      "epoch": 2.0680061823802163,
      "grad_norm": 0.2448832392692566,
      "learning_rate": 0.0001040045766590389,
      "loss": 0.0642,
      "step": 670
    },
    {
      "epoch": 2.098918083462133,
      "grad_norm": 0.4012124538421631,
      "learning_rate": 0.00010057208237986268,
      "loss": 0.061,
      "step": 680
    },
    {
      "epoch": 2.1298299845440494,
      "grad_norm": 0.380515992641449,
      "learning_rate": 9.71395881006865e-05,
      "loss": 0.062,
      "step": 690
    },
    {
      "epoch": 2.160741885625966,
      "grad_norm": 0.3869909346103668,
      "learning_rate": 9.370709382151028e-05,
      "loss": 0.0628,
      "step": 700
    },
    {
      "epoch": 2.1916537867078825,
      "grad_norm": 0.3945150375366211,
      "learning_rate": 9.027459954233408e-05,
      "loss": 0.0613,
      "step": 710
    },
    {
      "epoch": 2.2225656877897992,
      "grad_norm": 0.25586333870887756,
      "learning_rate": 8.68421052631579e-05,
      "loss": 0.0645,
      "step": 720
    },
    {
      "epoch": 2.2534775888717156,
      "grad_norm": 0.8637959957122803,
      "learning_rate": 8.340961098398168e-05,
      "loss": 0.0661,
      "step": 730
    },
    {
      "epoch": 2.2843894899536323,
      "grad_norm": 0.2937597930431366,
      "learning_rate": 7.997711670480547e-05,
      "loss": 0.06,
      "step": 740
    },
    {
      "epoch": 2.3153013910355487,
      "grad_norm": 0.22862955927848816,
      "learning_rate": 7.654462242562928e-05,
      "loss": 0.065,
      "step": 750
    },
    {
      "epoch": 2.346213292117465,
      "grad_norm": 0.3576946258544922,
      "learning_rate": 7.311212814645308e-05,
      "loss": 0.064,
      "step": 760
    },
    {
      "epoch": 2.3771251931993818,
      "grad_norm": 0.23851251602172852,
      "learning_rate": 6.967963386727689e-05,
      "loss": 0.0645,
      "step": 770
    },
    {
      "epoch": 2.4080370942812985,
      "grad_norm": 0.46925652027130127,
      "learning_rate": 6.624713958810069e-05,
      "loss": 0.0639,
      "step": 780
    },
    {
      "epoch": 2.438948995363215,
      "grad_norm": 0.2815486490726471,
      "learning_rate": 6.281464530892447e-05,
      "loss": 0.0694,
      "step": 790
    },
    {
      "epoch": 2.469860896445131,
      "grad_norm": 0.43563589453697205,
      "learning_rate": 5.938215102974828e-05,
      "loss": 0.0563,
      "step": 800
    },
    {
      "epoch": 2.500772797527048,
      "grad_norm": 0.2830500602722168,
      "learning_rate": 5.594965675057208e-05,
      "loss": 0.0627,
      "step": 810
    },
    {
      "epoch": 2.5316846986089647,
      "grad_norm": 0.2749677002429962,
      "learning_rate": 5.2517162471395874e-05,
      "loss": 0.0681,
      "step": 820
    },
    {
      "epoch": 2.562596599690881,
      "grad_norm": 0.37718528509140015,
      "learning_rate": 4.9084668192219674e-05,
      "loss": 0.0689,
      "step": 830
    },
    {
      "epoch": 2.5935085007727974,
      "grad_norm": 0.24426810443401337,
      "learning_rate": 4.5652173913043474e-05,
      "loss": 0.06,
      "step": 840
    },
    {
      "epoch": 2.624420401854714,
      "grad_norm": 0.29235589504241943,
      "learning_rate": 4.221967963386727e-05,
      "loss": 0.0611,
      "step": 850
    },
    {
      "epoch": 2.6553323029366305,
      "grad_norm": 0.8764016628265381,
      "learning_rate": 3.8787185354691075e-05,
      "loss": 0.0662,
      "step": 860
    },
    {
      "epoch": 2.6862442040185472,
      "grad_norm": 0.3143415153026581,
      "learning_rate": 3.535469107551487e-05,
      "loss": 0.0572,
      "step": 870
    },
    {
      "epoch": 2.7171561051004636,
      "grad_norm": 0.43903055787086487,
      "learning_rate": 3.192219679633867e-05,
      "loss": 0.0603,
      "step": 880
    },
    {
      "epoch": 2.7480680061823803,
      "grad_norm": 0.2884114384651184,
      "learning_rate": 2.848970251716247e-05,
      "loss": 0.0618,
      "step": 890
    },
    {
      "epoch": 2.7789799072642967,
      "grad_norm": 0.27862775325775146,
      "learning_rate": 2.5057208237986266e-05,
      "loss": 0.0626,
      "step": 900
    }
  ],
  "logging_steps": 10,
  "max_steps": 972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1982807230119936.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}

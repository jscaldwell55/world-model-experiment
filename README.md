World Model Experiment: Testing Interactive Learning in LLMs
Research Question: Can LLMs build better internal world models through interactive experience compared to pure language reasoning?

**NEW:** Token Prediction Bridge - Testing whether linguistic next-token prediction encodes similar learning signals as grounded world-model prediction. See TOKEN_EXPERIMENT_README.md for details.

Overview
This project implements a controlled experiment comparing how different LLM agent architectures perform on prediction and planning tasks:

Observer: Language-only reasoning, no interaction
Actor: Interactive agent that updates beliefs from experience
Text-Reader: Observer that reads prior episode logs (vicarious learning)
Model-Based: Actor + explicit learned dynamics model (MLP)

We test these agents in three micro-world environments designed to isolate specific reasoning capabilities:

Hot-Pot Lab: Causal reasoning with misleading linguistic priors
Switch-Light: Distinguishing intervention from observation (do-calculus)
Chem-Tile: Compositional reasoning with safety constraints

Dual API Architecture
This project uses a sophisticated dual API setup:

**Anthropic Claude Sonnet 4.5** (Primary - All Agent Operations)
- **Purpose:** All agent reasoning, belief updates, and decision-making
- **Why Claude?** Superior mathematical reasoning, long-context understanding, structured planning
- **Cost:** ~$3/1M input, $15/1M output tokens
- **Required:** Yes (for running experiments)

**OpenAI GPT-4o-mini** (Optional - Token Prediction Only)
- **Purpose:** Token-level log probabilities for coupling analysis
- **Why OpenAI?** Only provider offering token-level logprobs (required for NLL computation)
- **Cost:** ~$0.15/1M input, $0.60/1M output tokens
- **Required:** No (only for token prediction experiments)

This architecture provides the best of both worlds: Claude's exceptional reasoning for agents, and OpenAI's logprobs for linguistic analysis.

Key Design Principles
1. Scientific Rigor

Preregistration: Hypotheses locked in before experiments (preregistration.yaml)
Provenance: Every episode logs git SHA, code hashes, and full configuration
Reproducibility: Deterministic environments with explicit random seeds
Statistical power: Pre-computed sample sizes and effect sizes

2. Guard Rails Against Contamination

No ground truth leakage: Observations never contain hidden state
Programmatic injection: Observations injected into prompts, never echoed by LLM
Counterfactual purity: Simulation queries have no side effects
Validated observations: All observations pass through Pydantic schemas

3. Computable Metrics

Parametric beliefs: Agents maintain probability distributions (not "vibes")
Surprisal: -log P(observation | belief) computed from proper likelihoods
Calibration: Brier scores and Expected Calibration Error
Learning rate: Surprisal trajectory slope (negative = learning)

4. Transparency

Every component is identifiable: Single file per concept
No abstraction layers: Direct LLM API calls, no frameworks
JSON logs: Every episode fully inspectable and replayable
Version control: All prompts versioned, no magic strings

Project Structure
world-model-experiment/
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies
‚îú‚îÄ‚îÄ config.yaml                      # Model configs (Anthropic Claude), budgets, seeds
‚îú‚îÄ‚îÄ config_token.yaml                # Token prediction experiment configuration
‚îú‚îÄ‚îÄ preregistration.yaml             # Locked hypotheses (DO NOT MODIFY after experiments start)
‚îÇ
‚îú‚îÄ‚îÄ Documentation/
‚îÇ   ‚îú‚îÄ‚îÄ VALIDATION_REPORT.md         # System validation results (98% confidence)
‚îÇ   ‚îú‚îÄ‚îÄ BUG_FIX_SUMMARY.md           # Recent bug fixes and improvements
‚îÇ   ‚îú‚îÄ‚îÄ BUG_FIXES_CHEMTILE_SURPRISAL.md  # Detailed ChemTile bug fix documentation
‚îÇ   ‚îú‚îÄ‚îÄ TOKEN_EXPERIMENT_README.md   # Comprehensive token prediction guide
‚îÇ   ‚îú‚îÄ‚îÄ ANTHROPIC_MIGRATION.md       # Anthropic API migration summary
‚îÇ   ‚îî‚îÄ‚îÄ DIAGNOSTIC_REPORT.md         # System diagnostic results
‚îÇ
‚îú‚îÄ‚îÄ .env                             # API keys (gitignored, create this)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ test_*.py                        # Debug/validation scripts (temporary)
‚îÇ
‚îú‚îÄ‚îÄ environments/                    # Micro-world simulators
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      # Abstract Environment interface
‚îÇ   ‚îú‚îÄ‚îÄ hot_pot.py                   # Hot-Pot Lab (causal reasoning)
‚îÇ   ‚îú‚îÄ‚îÄ switch_light.py              # Switch-Light (intervention vs observation)
‚îÇ   ‚îú‚îÄ‚îÄ chem_tile.py                 # Chem-Tile (compositional reasoning)
‚îÇ   ‚îî‚îÄ‚îÄ transfer_env.py              # Out-of-distribution test
‚îÇ
‚îú‚îÄ‚îÄ agents/                          # Agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      # Abstract Agent + LLM interfaces (Anthropic + OpenAI)
‚îÇ   ‚îú‚îÄ‚îÄ observer.py                  # Language-only reasoning
‚îÇ   ‚îú‚îÄ‚îÄ actor.py                     # Interactive with belief updates
‚îÇ   ‚îú‚îÄ‚îÄ text_reader.py               # Reads prior logs
‚îÇ   ‚îî‚îÄ‚îÄ model_based.py               # Actor + learned dynamics
‚îÇ
‚îú‚îÄ‚îÄ models/                          # Belief states and tools
‚îÇ   ‚îú‚îÄ‚îÄ belief_state.py              # Parametric beliefs (HotPotBelief, etc)
‚îÇ   ‚îú‚îÄ‚îÄ transition_model.py          # MLP for dynamics learning
‚îÇ   ‚îî‚îÄ‚îÄ tools.py                     # Tool definitions per environment
‚îÇ
‚îú‚îÄ‚îÄ textualization/                  # Natural language conversion layer
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      # Abstract textualization interface
‚îÇ   ‚îú‚îÄ‚îÄ hot_pot_text.py              # HotPot observations ‚Üí natural language
‚îÇ   ‚îú‚îÄ‚îÄ switch_light_text.py         # SwitchLight observations ‚Üí natural language
‚îÇ   ‚îú‚îÄ‚îÄ chem_tile_text.py            # ChemTile observations ‚Üí natural language
‚îÇ   ‚îî‚îÄ‚îÄ validation.py                # Template validation (determinism, no leakage)
‚îÇ
‚îú‚îÄ‚îÄ token_prediction/                # Token-level prediction system
‚îÇ   ‚îú‚îÄ‚îÄ predictor.py                 # Abstract predictor interfaces
‚îÇ   ‚îú‚îÄ‚îÄ openai_predictor.py          # OpenAI logprobs implementation
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                    # Token prediction logging
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                   # Token-level metrics (NLL, perplexity)
‚îÇ
‚îú‚îÄ‚îÄ evaluation/                      # Metrics and analysis
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                   # All 7 core metrics (interventional accuracy, etc)
‚îÇ   ‚îú‚îÄ‚îÄ tasks.py                     # Test query sets
‚îÇ   ‚îú‚îÄ‚îÄ statistical.py               # Power analysis, t-tests, effect sizes
‚îÇ   ‚îú‚îÄ‚îÄ token_analysis.py            # Token prediction statistical analyses (A1-A5)
‚îÇ   ‚îî‚îÄ‚îÄ token_validation.py          # Token prediction robustness tests
‚îÇ
‚îú‚îÄ‚îÄ experiments/                     # Execution infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ runner.py                    # Main episode loop with guard rails
‚îÇ   ‚îú‚îÄ‚îÄ token_runner.py              # Episode runner with parallel token prediction
‚îÇ   ‚îú‚îÄ‚îÄ provenance.py                # Git SHA tracking, code hashing
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py                   # All prompts (versioned)
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # API key loading
‚îÇ   ‚îî‚îÄ‚îÄ ablations.py                 # Ablation configurations
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Entry points
‚îÇ   ‚îú‚îÄ‚îÄ run_experiment.py            # Main: run all episodes
‚îÇ   ‚îú‚îÄ‚îÄ analyze_results.py           # Generate report + figures
‚îÇ   ‚îú‚îÄ‚îÄ inspect_episode.py           # Debug single episode
‚îÇ   ‚îú‚îÄ‚îÄ compute_power_analysis.py    # Statistical power analysis
‚îÇ   ‚îú‚îÄ‚îÄ generate_all_logs.py         # Generate detailed logs for episodes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Token Prediction Scripts/
‚îÇ   ‚îú‚îÄ‚îÄ validate_templates.py        # Validate textualization templates
‚îÇ   ‚îú‚îÄ‚îÄ pilot_token_run.py           # Pilot token experiment (30 episodes)
‚îÇ   ‚îú‚îÄ‚îÄ analyze_token_pilot.py       # Pilot results analysis
‚îÇ   ‚îú‚îÄ‚îÄ run_full_token_experiment.py # Full token experiment (300 episodes)
‚îÇ   ‚îú‚îÄ‚îÄ analyze_full_token_results.py # Comprehensive token analysis (A1-A5)
‚îÇ   ‚îî‚îÄ‚îÄ generate_token_figures.py    # Publication-ready token figures
‚îÇ
‚îú‚îÄ‚îÄ results/                         # Generated outputs (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ raw/                         # JSON logs per episode
‚îÇ   ‚îú‚îÄ‚îÄ aggregated/                  # CSV summaries, figures
‚îÇ   ‚îú‚îÄ‚îÄ figures/                     # Plots
‚îÇ   ‚îî‚îÄ‚îÄ pilot_token*/                # Token prediction pilot results
‚îÇ
‚îú‚îÄ‚îÄ logs/                            # Human-readable episode logs (generated)
‚îÇ   ‚îî‚îÄ‚îÄ TIMESTAMP/                   # Timestamped log directories
‚îÇ
‚îî‚îÄ‚îÄ tests/                           # Test suite
    ‚îú‚îÄ‚îÄ conftest.py                  # Test configuration
    ‚îú‚îÄ‚îÄ test_environments.py         # Environment determinism, purity
    ‚îú‚îÄ‚îÄ test_agents.py               # Agent behavior
    ‚îú‚îÄ‚îÄ test_beliefs.py              # Likelihood computations
    ‚îú‚îÄ‚îÄ test_metrics.py              # Metric calculations
    ‚îú‚îÄ‚îÄ test_integration.py          # Full pipeline
    ‚îú‚îÄ‚îÄ test_textualization.py       # Textualization layer tests (16 tests)
    ‚îî‚îÄ‚îÄ test_token_prediction.py     # Token prediction tests (14 tests)
Quick Start
1. Install Dependencies
bashpip install -r requirements.txt
2. Configure API Keys
Create .env file in project root:
bashANTHROPIC_API_KEY=sk-ant-your-key-here  # Required: Used for all agent operations
OPENAI_API_KEY=sk-your-key-here             # Optional: Only needed for token prediction experiments

Important Notes:
- .env is gitignored. Never commit API keys.
- **Anthropic API (Required)**: All agents now use Claude Sonnet 4.5 for superior reasoning
- **OpenAI API (Optional)**: Only required for token prediction experiments (logprobs functionality)
- See ANTHROPIC_MIGRATION.md for complete migration details
3. Run Tests
Verify environments and metrics work:
bash# Test environments (no API calls needed)
pytest tests/test_environments.py -v

# Test metrics
pytest tests/test_metrics.py -v

# Integration test (requires API keys)
pytest tests/test_integration.py -v -m integration
4. Run Experiment
bashpython scripts/run_experiment.py \
    --config config.yaml \
    --preregistration preregistration.yaml
This will:

Run all environment √ó agent combinations
Save episode logs to results/raw/TIMESTAMP/
Track full provenance (git SHA, code versions)
Enforce all guard rails

5. Analyze Results
bashpython scripts/analyze_results.py \
    --results results/raw/TIMESTAMP/
Generates:

results/aggregated/metrics_summary.csv - Main results
results/aggregated/statistical_comparisons.csv - t-tests, effect sizes
results/aggregated/figures/ - Plots

6. Inspect Individual Episodes
bashpython scripts/inspect_episode.py \
    results/raw/TIMESTAMP/hot_pot_actor_ep001.json
Shows:

Step-by-step actions and observations
Surprisal trajectory
Belief state evolution
Test query performance

7. Generate Detailed Logs
bashpython scripts/generate_all_logs.py \
    results/raw/TIMESTAMP \
    --output-dir logs/TIMESTAMP
Generates human-readable text logs for all episodes with:

Full step-by-step trajectories
Observations, actions, and beliefs
Test query results
Metadata and ground truth

8. Compute Statistical Power
bashpython scripts/compute_power_analysis.py \
    --results results/aggregated/TIMESTAMP
Performs statistical power analysis for actor vs observer comparisons:

Effect sizes and confidence intervals
Required sample sizes for desired power
t-tests and statistical significance

Token-Level Prediction Bridge
This project includes experimental capability to test whether linguistic next-token prediction encodes similar learning signals as grounded world-model prediction (belief surprisal).
Quick Start
1. Validate Templates
bashpython scripts/validate_templates.py
2. Run Pilot (5 episodes √ó 3 envs √ó 2 agents = 30 episodes)
Requires OPENAI_API_KEY environment variable:
bashexport OPENAI_API_KEY='your-key-here'
python scripts/pilot_token_run.py
This will:

Convert environment observations to canonical natural language
Query LLMs for next-observation predictions with token logprobs
Compute token NLL alongside belief surprisal
Save token logs to results/raw/pilot_token_TIMESTAMP/

3. Analyze Results
bashpython scripts/analyze_token_pilot.py results/raw/pilot_token_TIMESTAMP
Generates:

Correlation analysis (token NLL vs belief surprisal)
Scatter plots showing coupling strength
CSV files with statistical results

Expected Pattern
The coupling between token NLL and belief surprisal should follow:

HotPot (strong coupling, r > 0.7): Causal dynamics well-captured by language
SwitchLight (moderate coupling, r ~ 0.5): Intervention reasoning partially linguistic
ChemTile (weak coupling, r < 0.4): Compositional reasoning requires interaction

Configuration
Edit config_token.yaml to adjust:

Model selection (GPT-4o, GPT-4o-mini, etc.)
Temperature settings
Number of pilot episodes
Budget constraints

Example:
yamltoken_prediction:
  enabled: true
  predictors:
    observer:
      model: "gpt-4o-mini"
      temperature: 0.0
    actor:
      model: "gpt-4o"
      temperature: 0.0

pilot:
  num_episodes_per_env: 5
  environments: ["hot_pot", "switch_light", "chem_tile"]
  agents: ["observer", "actor"]
  seeds: [42, 43, 44, 45, 46]
How It Works

Textualization Layer: Converts every environment observation into canonical natural language strings (deterministic 1:1 mapping)
Next-Sentence Prediction: Queries LLMs to predict the next observation text, capturing per-token log probabilities
Token NLL Computation: Calculates negative log-likelihood (NLL) = -Œ£ log p(token | context)
Alignment Analysis: Correlates token NLL with belief surprisal to see if they track together

Key Design Principles

Deterministic Templates: Same observation ‚Üí same text, always
No Ground Truth Leakage: Hidden state never appears in text
Synchronized Logging: Token NLL and belief surprisal recorded at same steps
Programmatic Injection: Text generated by code, not LLM (prevents hallucination)

Files

textualization/ - Converts observations to canonical text
token_prediction/ - OpenAI API integration for token logprobs
experiments/token_runner.py - Parallel token prediction during episodes
scripts/pilot_token_run.py - Pilot experiment runner
scripts/analyze_token_pilot.py - Coupling analysis and visualization
scripts/validate_templates.py - Template validation tests
config_token.yaml - Token prediction configuration

Core Concepts
Environments
Each environment is a deterministic simulator with:

Hidden state: Ground truth never shown to agents
Actions: Tools agents can use (measure, wait, toggle, etc)
Observations: What agents see (may include noise or misleading info)
Counterfactuals: Pure functions that simulate without side effects

Example from Hot-Pot Lab:
pythonenv = HotPotLab(seed=42)
obs = env.reset(42)  # {'label': 'Boiling!', 'stove_light': 'on'}

obs, reward, done, info = env.step('measure_temp()')
# obs = {'measured_temp': 23.5, 'time_elapsed': 0}

# Ground truth only for evaluation
gt = env.get_ground_truth()  
# {'actual_temp': 23.5, 'stove_power': 'off'}
Belief States
Agents maintain parametric probability distributions:
pythonfrom models.belief_state import HotPotBelief

belief = HotPotBelief(
    heating_rate_mean=1.5,    # ¬∞C per second
    heating_rate_std=0.3,     # Uncertainty
    measurement_noise=2.0     # Observation noise
)

# Compute likelihood for surprisal
log_prob = belief.log_likelihood(
    observation={'measured_temp': 45.0},
    time_elapsed=20.0
)

surprisal = -log_prob  # Higher = more surprising
Agents
All agents inherit from Agent base class:
pythonclass Agent(ABC):
    def act(self, observation: dict) -> AgentStep:
        """Process observation, return action"""
        
    def answer_query(self, question: str) -> Tuple[str, float]:
        """Answer query, return (answer, confidence)"""
Observer: Never acts, reasons from initial description only
Actor: Takes actions, updates beliefs, computes surprisal
Text-Reader: Like Observer but reads prior episode logs
Model-Based: Like Actor but also fits explicit MLP dynamics model
Metrics
Seven key metrics computed for each episode:

Interventional Accuracy: Correct answers on "What if we DO X?" queries
Counterfactual Accuracy: Correct answers on "What if we HAD DONE X?" queries
Surprisal Trajectory: Does surprisal decrease over time? (learning indicator)
Planning Success: Can agent achieve goals safely?
Calibration: Do confidence scores match actual accuracy?
Sample Efficiency: How many actions to reach target accuracy?
Œî Accuracy Post-Surprise: Does accuracy improve after surprising observations?

Experimental Hypotheses
From preregistration.yaml:
H1 (Primary): Actor agents achieve ‚â•15% higher interventional accuracy than Observers

Rationale: Interactive experience should build better causal models

H2: Actor agents show negative surprisal slope; Observers show flat slope

Rationale: Learning manifests as decreasing surprisal

H3: Model-Based agents outperform pure Actor reasoning by ‚â•10%

Rationale: Explicit dynamics models enable better planning

H4: Interventional accuracy improves after high-surprisal events

Rationale: Tests "observe ‚Üí update ‚Üí improve" loop

H5: Actors show better out-of-distribution generalization than Observers

Rationale: Experience-based models should transfer better

Configuration
Model Selection (config.yaml)
All agents now use Anthropic's Claude Sonnet 4.5 for superior reasoning capabilities:
yamlmodels:
  observer: "claude-sonnet-4-5-20250929"
  actor: "claude-sonnet-4-5-20250929"
  model_based: "claude-sonnet-4-5-20250929"
  text_reader: "claude-sonnet-4-5-20250929"

**Why Claude Sonnet 4.5?**
- Exceptional mathematical reasoning (belief state updates)
- Superior long-context understanding (episode histories)
- Strong structured reasoning (planning, counterfactuals)
- Excellent instruction following
- Pricing: $3/1M input tokens, $15/1M output tokens
Episode Budgets
yamlbudgets:
  actions_per_episode: 10    # Max actions per episode
  tokens_per_call: 2000       # Max tokens per LLM call
Seeds and Replication
yamlenvironments:
  hot_pot:
    num_episodes: 50
    seeds: [42, 43, 44, ...]  # Explicit seeds for reproducibility
Guard Rails and Safety
1. No Ground Truth Leakage
python# ‚úÖ Allowed
observation = {'measured_temp': 45.0, 'time_elapsed': 20}

# ‚ùå NEVER allowed - will trigger assertion
observation = {
    'measured_temp': 45.0, 
    'ground_truth': {'actual_temp': 45.0},  # FORBIDDEN
    'hidden_state': {...}                    # FORBIDDEN
}
2. Programmatic Observation Injection
python# In runner.py
agent_step = agent.act(observation)

# GUARD RAIL: Override with true observation
# (prevents LLM from hallucinating observations)
agent_step.observation = observation
3. Counterfactual Purity
python# Counterfactuals must not modify state
state_before = env.get_ground_truth()

result = env.counterfactual_query(['wait(60)', 'measure_temp'], seed=99)

state_after = env.get_ground_truth()
assert state_before == state_after  # Must be identical
4. Deterministic Environments
python# Same seed ‚Üí same trajectory
env1 = HotPotLab(seed=42)
env2 = HotPotLab(seed=42)

for _ in range(10):
    obs1, _, _, _ = env1.step('wait(5)')
    obs2, _, _, _ = env2.step('wait(5)')
    assert obs1 == obs2
Episode Log Format
Every episode saved as JSON with full provenance:
json{
  "episode_id": "hot_pot_actor_ep001",
  "seed": 42,
  "environment": "HotPotLab",
  "agent_type": "actor",
  "provenance": {
    "timestamp": "2024-01-15T14:30:22",
    "code_sha": "a3f9d2c1...",
    "environment_version": "e4b8c9a2...",
    "config_hash": "f1a2d5e3..."
  },
  "steps": [
    {
      "step_num": 0,
      "thought": "I should measure the temperature first",
      "action": "measure_temp()",
      "observation": {"measured_temp": 23.5, "time_elapsed": 0},
      "belief_state": {
        "heating_rate_mean": 1.5,
        "heating_rate_std": 0.3
      },
      "surprisal": 0.42,
      "token_usage": 150
    },
    ...
  ],
  "test_results": [
    {
      "query": "If we wait 30s and touch, what happens?",
      "query_type": "interventional",
      "agent_answer": "It would be safe, temp ~25¬∞C",
      "confidence": 0.85,
      "correct": true
    },
    ...
  ],
  "ground_truth": {
    "stove_power": "off",
    "actual_temp": 23.5
  }
}
Development Workflow
Adding a New Environment

Create environments/my_env.py inheriting from Environment
Implement all abstract methods (reset, step, get_ground_truth, counterfactual_query)
Create corresponding MyEnvBelief in models/belief_state.py
Add tools to models/tools.py
Write tests in tests/test_environments.py
Add test queries in evaluation/tasks.py

Adding a New Metric

Implement function in evaluation/metrics.py
Add to compute_all_metrics() aggregator
Write tests in tests/test_metrics.py
Update analysis script to plot/report the metric

Modifying Prompts
Never hardcode prompts in agent files!

Edit experiments/prompts.py
Update PROMPT_VERSION constant
Changes automatically logged in episode provenance

Recent Updates

Anthropic API Migration (October 2025) ‚úÖ COMPLETE
Successfully migrated all agent operations from OpenAI to Anthropic's Claude Sonnet 4.5. Token prediction continues to use OpenAI (required for logprobs). Key improvements:
- Superior reasoning capabilities for belief state updates
- Better long-context understanding
- Dual API architecture: Anthropic (agents) + OpenAI (token prediction only)
- See ANTHROPIC_MIGRATION.md for complete details

Token Prediction Bridge (October 2025) ‚úÖ IMPLEMENTED
Added experimental capability to test whether linguistic next-token prediction encodes similar learning signals as grounded world-model prediction (belief surprisal):
- New textualization/ layer: Converts observations to canonical natural language
- New token_prediction/ system: Queries LLMs for token-level logprobs
- 6 new scripts for running and analyzing token experiments
- Statistical analyses (A1-A5) for coupling metrics
- See TOKEN_EXPERIMENT_README.md for comprehensive guide

Action-Observation Alignment Fix (October 2025)
A critical bug in the episode runner has been fixed where actions and observations were misaligned by one step. The fix ensures that each logged step shows the action taken and the observation resulting from that action (not the previous observation). See BUG_FIX_SUMMARY.md for details.

System Validation (October 2025)
The entire system has been validated with 98% confidence. All mathematical formulas (surprisal, log-likelihood, belief updates) have been verified. The system is ready for full-scale experiments. See VALIDATION_REPORT.md for complete validation results.

Common Issues
"API key not found"
Solution: Create .env file with your keys:
bashANTHROPIC_API_KEY=sk-ant-...  # Required for all experiments
OPENAI_API_KEY=sk-...             # Only needed for token prediction

Note:
- Anthropic key is REQUIRED for all experiments
- OpenAI key is OPTIONAL (only for token prediction experiments)
"Counterfactual modified state"
Solution: Ensure counterfactual_query saves/restores state:
pythondef counterfactual_query(self, actions, seed):
    saved_state = self._save_state()
    # ... simulate ...
    self._restore_state(saved_state)
"Ground truth leaked to agent"
Solution: Check observation dict doesn't contain forbidden keys:
python# In environment.step()
obs = self._compute_observation()
assert 'ground_truth' not in obs
assert 'hidden_state' not in obs
return obs, reward, done, info
Tests failing with "No API keys"
Expected: Most tests don't need APIs. Only integration tests require keys.
bash# Run without API calls
pytest tests/test_environments.py tests/test_metrics.py -v

# Run with API calls (requires keys)
pytest tests/test_integration.py -v -m integration
Roadmap
Batch 1: Core Infrastructure ‚úÖ COMPLETE

‚úÖ Environments (hot_pot, switch_light, chem_tile)
‚úÖ Parametric belief states
‚úÖ Guard rails and provenance
‚úÖ Environment tests

Batch 2: Agents ‚úÖ COMPLETE

‚úÖ Observer agent
‚úÖ Actor agent with belief updates
‚úÖ Text-Reader baseline
‚úÖ Model-Based agent
‚úÖ Transition model (MLP)
‚úÖ Agent tests

Batch 3: Evaluation & Execution ‚úÖ COMPLETE

‚úÖ Test query sets
‚úÖ All 7 metrics
‚úÖ Experiment runner
‚úÖ Analysis script
‚úÖ Visualization
‚úÖ Integration tests
‚úÖ Statistical power analysis
‚úÖ Episode log generation

Batch 4: Token Prediction Bridge ‚úÖ COMPLETE

‚úÖ Textualization layer (deterministic, no leakage)
‚úÖ OpenAI token prediction integration
‚úÖ Parallel episode + token logging
‚úÖ Statistical analyses (A1-A5)
‚úÖ Validation tests (16 textualization + 14 token prediction)
‚úÖ Pilot and full experiment scripts
‚úÖ Token analysis and figure generation

Batch 5: API Migration ‚úÖ COMPLETE

‚úÖ Anthropic Claude Sonnet 4.5 integration
‚úÖ Dual API architecture (Anthropic + OpenAI)
‚úÖ All tests passing with Claude
‚úÖ Migration documentation

Current Status: üöÄ READY FOR EXPERIMENTS

System validated (98% confidence) and ready for full-scale experiments with:
- All agents running on Anthropic Claude Sonnet 4.5
- Optional token prediction using OpenAI (for logprobs)
- Complete test coverage
- Comprehensive documentation

**Documentation:**
- VALIDATION_REPORT.md - System validation results
- BUG_FIX_SUMMARY.md - Recent bug fixes
- TOKEN_EXPERIMENT_README.md - Token prediction guide
- ANTHROPIC_MIGRATION.md - API migration details

Next Steps

1. **Run Full Experiment** (300 episodes across 3 environments √ó 4 agents)
2. **Statistical Analysis** (Test all 5 preregistered hypotheses)
3. **Token Coupling Analysis** (A1-A5 statistical tests)
4. **Publication** (Write up results)

Future Research Directions

‚ñ° Transfer environment experiments (out-of-distribution generalization)
‚ñ° Model family comparison (GPT-4 vs Claude vs Llama)
‚ñ° Context length ablations (full history vs last N steps)
‚ñ° Temperature sensitivity analysis
‚ñ° Prompt engineering variants (CoT, few-shot)
‚ñ° Alternative belief representations (neural, symbolic hybrid)
‚ñ° Advanced planning algorithms (MCTS, value iteration)

---

Documentation Index

**Core Documentation:**
- **README.md** (this file) - Main project overview and quick start
- **preregistration.yaml** - Locked experimental hypotheses (H1-H5)
- **config.yaml** - Model configurations and experiment settings
- **config_token.yaml** - Token prediction experiment settings

**Feature Guides:**
- **TOKEN_EXPERIMENT_README.md** - Comprehensive guide to token prediction experiments
  - How token prediction works
  - Running pilot and full experiments
  - Statistical analyses (A1-A5)
  - Cost estimates and troubleshooting
  - 20+ pages of detailed documentation

**Technical Reports:**
- **VALIDATION_REPORT.md** - System validation results (98% confidence)
  - Mathematical formula verification
  - Belief state validation
  - Surprisal computation validation
- **ANTHROPIC_MIGRATION.md** - API migration summary
  - Dual API architecture details
  - Migration checklist
  - Cost comparisons
  - Testing results

**Bug Fixes and Diagnostics:**
- **BUG_FIX_SUMMARY.md** - Recent bug fixes overview
- **BUG_FIXES_CHEMTILE_SURPRISAL.md** - Detailed ChemTile bug fix documentation
- **DIAGNOSTIC_REPORT.md** - System diagnostic results

**Quick Reference:**

| Task | Documentation |
|------|---------------|
| Getting started | README.md Quick Start section |
| Understanding hypotheses | preregistration.yaml |
| Running core experiments | README.md ‚Üí sections 4-6 |
| Token prediction setup | TOKEN_EXPERIMENT_README.md |
| API configuration | ANTHROPIC_MIGRATION.md |
| Troubleshooting | README.md Common Issues, TOKEN_EXPERIMENT_README.md Troubleshooting |
| System validation | VALIDATION_REPORT.md |
| Understanding architectures | README.md ‚Üí Dual API Architecture section |

**Test Coverage:**
- environments: 100% (test_environments.py)
- agents: 100% (test_agents.py)
- beliefs: 100% (test_beliefs.py)
- metrics: 100% (test_metrics.py)
- textualization: 100% (test_textualization.py - 16 tests)
- token prediction: 100% (test_token_prediction.py - 14 tests)
- integration: Full pipeline (test_integration.py)

---

**Questions or Issues?**
1. Check the relevant documentation file above
2. Review Common Issues section in this README
3. See TOKEN_EXPERIMENT_README.md for token-specific questions
4. Check git history for implementation details

**Citation:**
If you use this codebase in your research, please cite appropriately and reference the preregistration.yaml for experimental hypotheses.
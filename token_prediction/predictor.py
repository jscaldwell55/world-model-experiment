"""Abstract interfaces for token-level prediction."""

from abc import ABC, abstractmethod
from typing import List, Dict, Tuple
from dataclasses import dataclass


@dataclass
class TokenPrediction:
    """Result of token-level prediction.

    Attributes:
        predicted_text: The predicted text generated by the model
        tokens: List of tokens in the prediction
        logprobs: List of log probabilities for each token
        sequence_nll: Negative log-likelihood of the sequence (-sum(logprobs))
        per_token_nll: Normalized NLL per token (sequence_nll / len(tokens))
    """
    predicted_text: str
    tokens: List[str]
    logprobs: List[float]
    sequence_nll: float
    per_token_nll: float

    def __post_init__(self):
        """Validate that dimensions match."""
        if len(self.tokens) != len(self.logprobs):
            raise ValueError(
                f"Tokens and logprobs must have same length: "
                f"{len(self.tokens)} vs {len(self.logprobs)}"
            )

        # Recompute NLL metrics to ensure consistency
        if self.logprobs:
            self.sequence_nll = -sum(self.logprobs)
            self.per_token_nll = self.sequence_nll / len(self.logprobs)
        else:
            self.sequence_nll = 0.0
            self.per_token_nll = 0.0


class NextSentencePredictor(ABC):
    """Interface for LLM next-sentence prediction with logprobs.

    This abstract class defines the interface for querying language models
    to predict the next observation in an environment transcript, along with
    token-level log probabilities for uncertainty quantification.
    """

    @abstractmethod
    def predict_next_observation(
        self,
        context: str,
        temperature: float = 0.0,
        max_tokens: int = 100
    ) -> TokenPrediction:
        """Predict next observation and return token logprobs.

        Args:
            context: Full transcript up to current step
            temperature: Sampling temperature (0.0 for deterministic)
            max_tokens: Maximum tokens to generate

        Returns:
            TokenPrediction with predicted text and logprobs

        Raises:
            RuntimeError: If API call fails
        """
        pass

    @abstractmethod
    def rank_candidates(
        self,
        context: str,
        candidates: List[str]
    ) -> List[Tuple[str, float]]:
        """Score multiple candidate observations.

        Args:
            context: Full transcript up to current step
            candidates: List of candidate observation strings

        Returns:
            List of (candidate, nll) tuples sorted by NLL (lower is better)

        Raises:
            RuntimeError: If API call fails
        """
        pass

    def get_model_name(self) -> str:
        """Get the name of the underlying model.

        Returns:
            Model name string (e.g., "gpt-4o-mini")
        """
        return "unknown"

    def get_provider(self) -> str:
        """Get the provider name.

        Returns:
            Provider name string (e.g., "openai", "anthropic")
        """
        return "unknown"
